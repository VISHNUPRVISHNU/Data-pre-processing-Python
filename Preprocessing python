from nltk.stem.porter import *
import nltk
import pandas as pd
stop_words=nltk.corpus.stopwords.words("english")
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
import re
stemmer = PorterStemmer()
data= pd.read_csv("DatasetFile.csv")
data.col_name=data.col_name.apply(str)

data['col_name'] = data['col_name'].apply(lambda x: x.lower())
data['col_name'] = data['col_name'].apply((lambda x: re.sub(r'http\S+','',x)))
data['col_name'] = data['col_name'].apply((lambda x: re.sub('[^a-zA-z\s]','',x)))
data['col_name'] = data['col_name'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))
data['col_name'] = data['col_name'].apply(lambda x: ' '.join([w for w in x.split() if w not in stop_words]))
tokens = tokens.apply(lambda x: [stemmer.stem(i) for i in x]) 
for i in range(len(tokens)):
     tokens[i] = ' '.join(tokens[i])
data['col_name'] = tokens  
import pandas as pd 
data = data[['col_name','class']]
data.col_name=data.col_name.apply(str)
data['col_name'] = data['col_name'].apply(lambda x: x.lower())
data['col_name'] = data['col_name'].apply((lambda x: re.sub('[^a-zA-z0-9\s]','',x)))
print("len",len(data['col_name']))
t = Tokenizer(split=' ')
t.fit_on_texts(data['col_name'].values)
X = t.texts_to_sequences(data['col_name'].values)
X = pad_sequences(X)
Y = pd.get_dummies(data['class']).values
X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = 41)
